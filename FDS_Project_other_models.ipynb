{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Below code is for other models - MultiChannelCNN, LSTM and BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Data/train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [2], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msequence\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pad_sequences\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Load the dataset\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m train_data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mData/train.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m test_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData/test.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Data Preprocessing\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/FDS_310/lib/python3.10/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/FDS_310/lib/python3.10/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/FDS_310/lib/python3.10/site-packages/pandas/io/parsers/readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m    947\u001b[0m )\n\u001b[1;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/FDS_310/lib/python3.10/site-packages/pandas/io/parsers/readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    602\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    604\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 605\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/opt/anaconda3/envs/FDS_310/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1439\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/FDS_310/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1734\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1736\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1737\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1738\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1739\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1740\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1741\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1742\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1743\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/opt/anaconda3/envs/FDS_310/lib/python3.10/site-packages/pandas/io/common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    851\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    852\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    853\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    854\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    855\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 856\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    863\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    864\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    865\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Data/train.csv'"
     ]
    }
   ],
   "source": [
    "# MultiChannelCNN\n",
    "\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Embedding, Conv1D, GlobalMaxPooling1D, concatenate, Dense, Dropout\n",
    ")\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, Callback\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Load the dataset\n",
    "train_data = pd.read_csv(\"Data/train.csv\")\n",
    "test_data = pd.read_csv(\"Data/test.csv\")\n",
    "\n",
    "# Data Preprocessing\n",
    "def preprocess_text(df):\n",
    "    df['keyword'] = df['keyword'].fillna('')\n",
    "    df['location'] = df['location'].fillna('')\n",
    "    df['text'] = df['text'].fillna('')\n",
    "    df['combined_text'] = df['keyword'] + \" \" + df['location'] + \" \" + df['text']\n",
    "    df['combined_text'] = df['combined_text'].str.replace(r'http\\S+', '', regex=True)\n",
    "    df['combined_text'] = df['combined_text'].str.replace(r'@\\w+', '', regex=True)\n",
    "    df['combined_text'] = df['combined_text'].str.replace(r'#', '', regex=True)\n",
    "    return df\n",
    "\n",
    "train_data = preprocess_text(train_data)\n",
    "test_data = preprocess_text(test_data)\n",
    "\n",
    "# Tokenization and Padding\n",
    "max_words = 20000\n",
    "max_len = 100\n",
    "embedding_dim = 128  # Embedding dimension for trainable embeddings\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(train_data['combined_text'])\n",
    "\n",
    "X = tokenizer.texts_to_sequences(train_data['combined_text'])\n",
    "X = pad_sequences(X, maxlen=max_len)\n",
    "y = train_data['target']\n",
    "\n",
    "test_sequences = tokenizer.texts_to_sequences(test_data['combined_text'])\n",
    "test_padded = pad_sequences(test_sequences, maxlen=max_len)\n",
    "\n",
    "# Train-Test Split\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Custom Callback to Compute F1-Score During Training\n",
    "class F1ScoreCallback(Callback):\n",
    "    def __init__(self, validation_data):\n",
    "        self.validation_data = validation_data\n",
    "        self.f1_scores = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        X_val, y_val = self.validation_data\n",
    "        y_pred_prob = self.model.predict(X_val).flatten()\n",
    "        y_pred = (y_pred_prob > 0.5).astype(int)\n",
    "        f1 = f1_score(y_val, y_pred)\n",
    "        self.f1_scores.append(f1)\n",
    "        print(f\"Epoch {epoch + 1} F1-Score: {f1:.4f}\")\n",
    "\n",
    "# Hyperparameters\n",
    "filters = 256\n",
    "kernel_sizes = [3, 5, 7]\n",
    "dropout_rate = 0.75\n",
    "learning_rate = 0.00005\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "\n",
    "# Build the Multi-Channel CNN Model\n",
    "input_layer = Input(shape=(max_len,))\n",
    "embedding_layer = Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_len)(input_layer)\n",
    "\n",
    "# Convolutional layers with multiple kernel sizes\n",
    "conv_blocks = []\n",
    "for kernel_size in kernel_sizes:\n",
    "    conv = Conv1D(filters=filters, kernel_size=kernel_size, activation='relu')(embedding_layer)\n",
    "    pool = GlobalMaxPooling1D()(conv)\n",
    "    conv_blocks.append(pool)\n",
    "\n",
    "# Concatenate pooled outputs\n",
    "concat = concatenate(conv_blocks)\n",
    "\n",
    "# Fully connected layers\n",
    "dense1 = Dense(128, activation='relu')(concat)\n",
    "dropout = Dropout(dropout_rate)(dense1)\n",
    "output_layer = Dense(1, activation='sigmoid')(dropout)\n",
    "\n",
    "# Define and Compile the Model\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "model.compile(optimizer=Adam(learning_rate=learning_rate), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Summary of the model\n",
    "model.summary()\n",
    "\n",
    "# Early stopping and F1-score callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "f1_callback = F1ScoreCallback(validation_data=(X_valid, y_valid))\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    callbacks=[early_stopping, f1_callback],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate the Model\n",
    "y_pred_prob = model.predict(X_valid).flatten()\n",
    "y_pred = (y_pred_prob > 0.5).astype(int)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_valid, y_pred))\n",
    "\n",
    "# Plot Training History (Accuracy, Loss, and F1-Score)\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.title(\"Training and Validation Accuracy\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Training and Validation Loss\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(f1_callback.f1_scores, label='Validation F1-Score')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"F1-Score\")\n",
    "plt.legend()\n",
    "plt.title(\"Validation F1-Score per Epoch\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  RNN and LSTM\n",
    "\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, Callback\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "# Refined Hyperparameters\n",
    "MAX_WORDS = 20000         # Maximum number of words in the vocabulary\n",
    "MAX_LEN = 100             # Maximum length of input sequences\n",
    "EMBEDDING_DIM = 128       # Dimensionality of the embedding layer\n",
    "LSTM_UNITS_1 = 64         # Reduced units in the first LSTM layer\n",
    "LSTM_UNITS_2 = 32         # Reduced units in the second LSTM layer\n",
    "DENSE_UNITS = 32          # Reduced units in the dense layer\n",
    "DROPOUT_RATE = 0.70        # Increased dropout rate for regularization\n",
    "LEARNING_RATE = 0.00007    # Reduced learning rate for finer updates\n",
    "BATCH_SIZE = 32           # Batch size for training\n",
    "EPOCHS = 5                # Number of training epochs\n",
    "EARLY_STOPPING_PATIENCE = 2  # Early stopping patience to prevent overfitting\n",
    "\n",
    "# Load the dataset\n",
    "train_data = pd.read_csv(\"Data/train.csv\")\n",
    "test_data = pd.read_csv(\"Data/test.csv\")\n",
    "\n",
    "# Data Preprocessing\n",
    "def preprocess_text(df):\n",
    "    df['keyword'] = df['keyword'].fillna('')\n",
    "    df['location'] = df['location'].fillna('')\n",
    "    df['text'] = df['text'].fillna('')\n",
    "    df['combined_text'] = df['keyword'] + \" \" + df['location'] + \" \" + df['text']\n",
    "    df['combined_text'] = df['combined_text'].str.replace(r'http\\S+', '', regex=True)\n",
    "    df['combined_text'] = df['combined_text'].str.replace(r'@\\w+', '', regex=True)\n",
    "    df['combined_text'] = df['combined_text'].str.replace(r'#', '', regex=True)\n",
    "    return df\n",
    "\n",
    "train_data = preprocess_text(train_data)\n",
    "test_data = preprocess_text(test_data)\n",
    "\n",
    "# Tokenization and Padding\n",
    "tokenizer = Tokenizer(num_words=MAX_WORDS)\n",
    "tokenizer.fit_on_texts(train_data['combined_text'])\n",
    "\n",
    "X = tokenizer.texts_to_sequences(train_data['combined_text'])\n",
    "X = pad_sequences(X, maxlen=MAX_LEN)\n",
    "y = train_data['target']\n",
    "\n",
    "test_sequences = tokenizer.texts_to_sequences(test_data['combined_text'])\n",
    "test_padded = pad_sequences(test_sequences, maxlen=MAX_LEN)\n",
    "\n",
    "# Train-Test Split\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Custom Callback to Compute F1-Score During Training\n",
    "class F1ScoreCallback(Callback):\n",
    "    def __init__(self, validation_data):\n",
    "        self.validation_data = validation_data\n",
    "        self.f1_scores = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        X_val, y_val = self.validation_data\n",
    "        y_pred_prob = self.model.predict(X_val).flatten()\n",
    "        y_pred = (y_pred_prob > 0.5).astype(int)\n",
    "        f1 = f1_score(y_val, y_pred)\n",
    "        self.f1_scores.append(f1)\n",
    "        print(f\"Epoch {epoch + 1} F1-Score: {f1:.4f}\")\n",
    "\n",
    "# Build the Refined RNN with LSTM Model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=MAX_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_LEN))\n",
    "model.add(Bidirectional(LSTM(LSTM_UNITS_1, return_sequences=True, dropout=0.3, recurrent_dropout=0.3)))\n",
    "model.add(Bidirectional(LSTM(LSTM_UNITS_2, dropout=0.3, recurrent_dropout=0.3)))\n",
    "model.add(Dense(DENSE_UNITS, activation='relu', kernel_regularizer=l2(0.01)))  # Added L2 regularization\n",
    "model.add(Dropout(DROPOUT_RATE))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the Model\n",
    "model.compile(optimizer=Adam(learning_rate=LEARNING_RATE), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Summary of the Model\n",
    "model.summary()\n",
    "\n",
    "# Early stopping and F1-score callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=EARLY_STOPPING_PATIENCE, restore_best_weights=True)\n",
    "f1_callback = F1ScoreCallback(validation_data=(X_valid, y_valid))\n",
    "\n",
    "# Train the Model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=[early_stopping, f1_callback],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate the Model\n",
    "y_pred_prob = model.predict(X_valid).flatten()\n",
    "y_pred = (y_pred_prob > 0.5).astype(int)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_valid, y_pred))\n",
    "\n",
    "# Plot Training History (Accuracy, Loss, and F1-Score)\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.title(\"Training and Validation Accuracy\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Training and Validation Loss\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(f1_callback.f1_scores, label='Validation F1-Score')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"F1-Score\")\n",
    "plt.legend()\n",
    "plt.title(\"Validation F1-Score per Epoch\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT\n",
    "\n",
    "\n",
    "# pip install transformers datasets scikit-learn torch tqdm matplotlib\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import AdamW, get_scheduler\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Hyperparameters\n",
    "MODEL_NAME = \"bert-base-uncased\"  # Pretrained BERT model\n",
    "MAX_LEN = 128                    # Maximum sequence length\n",
    "BATCH_SIZE = 16                  # Batch size\n",
    "LEARNING_RATE = 2e-6             # Learning rate\n",
    "EPOCHS = 3                       # Number of training epochs\n",
    "EPS = 1e-8                       # AdamW optimizer epsilon\n",
    "\n",
    "# Load the dataset\n",
    "train_data = pd.read_csv(\"Data/train.csv\")\n",
    "test_data = pd.read_csv(\"Data/test.csv\")\n",
    "\n",
    "# Preprocessing\n",
    "def preprocess_text(df):\n",
    "    df['keyword'] = df['keyword'].fillna('')\n",
    "    df['location'] = df['location'].fillna('')\n",
    "    df['text'] = df['text'].fillna('')\n",
    "    df['combined_text'] = df['keyword'] + \" \" + df['location'] + \" \" + df['text']\n",
    "    return df\n",
    "\n",
    "train_data = preprocess_text(train_data)\n",
    "test_data = preprocess_text(test_data)\n",
    "\n",
    "# Split into train and validation sets\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    train_data['combined_text'], train_data['target'], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Tokenizer and Model Initialization\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Dataset Class\n",
    "class TweetDataset(Dataset):\n",
    "    def __init__(self, texts, labels=None, tokenizer=None, max_len=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = str(self.texts[index])\n",
    "        inputs = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_len,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        item = {key: val.squeeze(0) for key, val in inputs.items()}\n",
    "        if self.labels is not None:\n",
    "            item[\"labels\"] = torch.tensor(self.labels[index], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "# Create Dataset and DataLoader\n",
    "train_dataset = TweetDataset(X_train.tolist(), y_train.tolist(), tokenizer, max_len=MAX_LEN)\n",
    "valid_dataset = TweetDataset(X_valid.tolist(), y_valid.tolist(), tokenizer, max_len=MAX_LEN)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "# Load Pretrained BERT Model\n",
    "model = BertForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)\n",
    "model = model.to(device)\n",
    "\n",
    "# Optimizer and Scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, eps=EPS)\n",
    "num_training_steps = len(train_dataloader) * EPOCHS\n",
    "lr_scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
    "\n",
    "# Enhanced Training Loop with Training and Validation Metrics Tracking\n",
    "def train_model(model, train_dataloader, valid_dataloader, optimizer, lr_scheduler, device, epochs=3):\n",
    "    training_loss_history = []\n",
    "    validation_loss_history = []\n",
    "    training_accuracy_history = []\n",
    "    validation_accuracy_history = []\n",
    "    training_f1_history = []\n",
    "    validation_f1_history = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "        \n",
    "        # Training Phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_preds = []\n",
    "        train_labels = []\n",
    "        loop = tqdm(train_dataloader, leave=True)\n",
    "        for batch in loop:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            train_loss += loss.item()\n",
    "            \n",
    "            logits = outputs.logits\n",
    "            train_preds.extend(torch.argmax(logits, dim=1).cpu().numpy())\n",
    "            train_labels.extend(batch[\"labels\"].cpu().numpy())\n",
    "            \n",
    "            loop.set_description(f\"Epoch {epoch + 1}\")\n",
    "            loop.set_postfix(loss=loss.item())\n",
    "        \n",
    "        # Compute Training Metrics\n",
    "        training_loss_history.append(train_loss / len(train_dataloader))\n",
    "        train_accuracy = np.mean(np.array(train_preds) == np.array(train_labels))\n",
    "        train_f1 = classification_report(train_labels, train_preds, output_dict=True)['weighted avg']['f1-score']\n",
    "        training_accuracy_history.append(train_accuracy)\n",
    "        training_f1_history.append(train_f1)\n",
    "        print(f\"Training Loss: {train_loss / len(train_dataloader):.4f}\")\n",
    "        print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
    "        print(f\"Training F1-Score: {train_f1:.4f}\")\n",
    "\n",
    "        # Validation Phase\n",
    "        model.eval()\n",
    "        valid_loss = 0.0\n",
    "        valid_preds = []\n",
    "        valid_labels = []\n",
    "        with torch.no_grad():\n",
    "            for batch in valid_dataloader:\n",
    "                batch = {k: v.to(device) for k, v in batch.items()}\n",
    "                outputs = model(**batch)\n",
    "                valid_loss += outputs.loss.item()\n",
    "                logits = outputs.logits\n",
    "                valid_preds.extend(torch.argmax(logits, dim=1).cpu().numpy())\n",
    "                valid_labels.extend(batch[\"labels\"].cpu().numpy())\n",
    "        \n",
    "        # Compute Validation Metrics\n",
    "        validation_loss_history.append(valid_loss / len(valid_dataloader))\n",
    "        valid_accuracy = np.mean(np.array(valid_preds) == np.array(valid_labels))\n",
    "        valid_f1 = classification_report(valid_labels, valid_preds, output_dict=True)['weighted avg']['f1-score']\n",
    "        validation_accuracy_history.append(valid_accuracy)\n",
    "        validation_f1_history.append(valid_f1)\n",
    "        print(f\"Validation Loss: {valid_loss / len(valid_dataloader):.4f}\")\n",
    "        print(f\"Validation Accuracy: {valid_accuracy:.4f}\")\n",
    "        print(f\"Validation F1-Score: {valid_f1:.4f}\")\n",
    "\n",
    "    return (\n",
    "        training_loss_history,\n",
    "        validation_loss_history,\n",
    "        training_accuracy_history,\n",
    "        validation_accuracy_history,\n",
    "        training_f1_history,\n",
    "        validation_f1_history,\n",
    "    )\n",
    "\n",
    "# Train the Model\n",
    "(\n",
    "    training_loss_history,\n",
    "    validation_loss_history,\n",
    "    training_accuracy_history,\n",
    "    validation_accuracy_history,\n",
    "    training_f1_history,\n",
    "    validation_f1_history,\n",
    ") = train_model(model, train_dataloader, valid_dataloader, optimizer, lr_scheduler, device, epochs=EPOCHS)\n",
    "\n",
    "# Plot Training and Validation Loss\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(training_loss_history, label='Train Loss')\n",
    "plt.plot(validation_loss_history, label='Validation Loss')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Training and Validation Loss\")\n",
    "plt.show()\n",
    "\n",
    "# Plot Training and Validation Accuracy\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(training_accuracy_history, label='Train Accuracy')\n",
    "plt.plot(validation_accuracy_history, label='Validation Accuracy')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.title(\"Training and Validation Accuracy\")\n",
    "plt.show()\n",
    "\n",
    "# Plot Training and Validation F1-Score\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(training_f1_history, label='Train F1-Score')\n",
    "plt.plot(validation_f1_history, label='Validation F1-Score')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"F1-Score\")\n",
    "plt.legend()\n",
    "plt.title(\"Training and Validation F1-Score\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
